{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWg14xyOL2OG"
   },
   "source": [
    "# HOMEWORK 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVk0oUzwEgBx"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0_1MRMKDHb6f"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_data():\n",
    "    (train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "    return (train_ds, test_ds), ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FAd7Erb2EQBN"
   },
   "outputs": [],
   "source": [
    "def new_target_fnc(ds, window_size):\n",
    "  l = list()\n",
    "  for i, elem in enumerate(ds):\n",
    "    if (i % window_size) == 0:\n",
    "      l.append(int(elem[1]))\n",
    "    else:\n",
    "      if (i % 2) == 0:\n",
    "        l.append(int(l[i-1] + elem[1]))\n",
    "      else:\n",
    "        l.append(int(l[i-1] - elem[1]))\n",
    "  return l\n",
    "\n",
    "def preprocess(data, batch_size, window_size):\n",
    "  # replacing the target with a new target\n",
    "  new_targets = new_target_fnc(data, window_size)\n",
    "  new_targets = tf.data.Dataset.from_tensor_slices(new_targets)\n",
    "  data = tf.data.Dataset.zip((data, new_targets))\n",
    "  data = data.map(lambda img, new_target: (img[0], new_target))\n",
    "\n",
    "  # convert (uint8 to float32)\n",
    "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
    "  # create one-hot targets (39 classes from -18 to 18)\n",
    "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=39)))\n",
    "  \n",
    "  data = tf.data.Dataset.window(data, window_size)\n",
    "  # caching transformation on dataset \n",
    "  data = data.cache() \n",
    "\n",
    "  # data = data.shuffle(1000) # Does it shuffle whole points or windows?\n",
    "  data = data.batch(batch_size)\n",
    "  \n",
    "  data = data.prefetch(tf.data.AUTOTUNE)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "  def __init__(self, optimizer, loss_function, input_shape):\n",
    "    super().__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(28, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv2 = tf.keras.layers.Conv2D(28, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.pooling1 = tf.keras.layers.AveragePooling2D()\n",
    "    self.conv3 = tf.keras.layers.Conv2D(14, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv4 = tf.keras.layers.Conv2D(14, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.pooling2 = tf.keras.layers.AveragePooling2D()\n",
    "    self.conv5 = tf.keras.layers.Conv2D(7, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv6 = tf.keras.layers.Conv2D(7, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
    "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "    self.metrics_list = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.Mean(name=\"loss\")\n",
    "    ]\n",
    "\n",
    "  def call(self, x, training=False):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pooling1(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.pooling2(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.conv6(x)\n",
    "    x = self.globalpooling(x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "\n",
    "  # reset all metrics\n",
    "  def reset_metrics(self):\n",
    "      for metric in self.metrics:\n",
    "          metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "      image, label = data\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "          prediction = self(image, training = True)\n",
    "          loss = self.loss_function(label, prediction)\n",
    "\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "      image, label = data\n",
    "      prediction = self(image, training = False)\n",
    "      loss = self.loss_function(label, prediction)\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range (epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            model.train_step(data)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in test_ds:\n",
    "            model.test_step(data)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "    \n",
    "    model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pKg9wCJo11zO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]2022-12-11 21:31:53.959461: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "  0%|          | 0/469 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_35194/4221311034.py\", line 46, in train_step  *\n        prediction = self(image, training = True)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"cnn_3\" (type CNN).\n    \n    in user code:\n    \n        File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_35194/4221311034.py\", line 24, in call  *\n            x = self.conv1(x)\n        File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 197, in assert_input_compatibility\n            raise TypeError(f'Inputs to a layer should be tensors. Got: {x}')\n    \n        TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x120837580>\n    \n    \n    Call arguments received:\n      • x=<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x120837580>\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m train_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(train_log_path)\n\u001b[1;32m     17\u001b[0m test_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(test_log_path)\n\u001b[0;32m---> 18\u001b[0m training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\n",
      "Cell \u001b[0;32mIn[21], line 68\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m model\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 68\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     71\u001b[0m     tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname, model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_35194/4221311034.py\", line 46, in train_step  *\n        prediction = self(image, training = True)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"cnn_3\" (type CNN).\n    \n    in user code:\n    \n        File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_35194/4221311034.py\", line 24, in call  *\n            x = self.conv1(x)\n        File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 197, in assert_input_compatibility\n            raise TypeError(f'Inputs to a layer should be tensors. Got: {x}')\n    \n        TypeError: Inputs to a layer should be tensors. Got: <tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x120837580>\n    \n    \n    Call arguments received:\n      • x=<tensorflow.python.data.ops.dataset_ops._NestedVariant object at 0x120837580>\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "window_size = 4\n",
    "(train_ds,test_ds), ds_info = load_data()\n",
    "train_ds = preprocess(train_ds, batch_size, window_size)\n",
    "test_ds = preprocess(test_ds, batch_size, window_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "cnn = CNN(optimizer=optimizer, loss_function=loss_function, input_shape=(batch_size, window_size, 28, 28, 1))\n",
    "epochs = 10\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"models/{current_time}\"\n",
    "train_log_path = f\"logs/{current_time}/train\"\n",
    "test_log_path = f\"logs/{current_time}/test\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('scipy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6afa05d019bf0021cfdd951a280579caaf080683501157ada207acf29e18cb37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
