{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Efefefef/IANNwTF-UniOsnabrueck/blob/main/homework07/homework07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWg14xyOL2OG"
      },
      "source": [
        "# HOMEWORK 07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 614,
      "metadata": {
        "id": "9R9hKeIHyZDX"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell\n",
        "from keras.initializers import Orthogonal\n",
        "from tqdm import tqdm\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVk0oUzwEgBx"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 615,
      "metadata": {
        "id": "0_1MRMKDHb6f"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "def load_data():\n",
        "    (train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "    return (train_ds, test_ds), ds_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 616,
      "metadata": {
        "id": "FAd7Erb2EQBN"
      },
      "outputs": [],
      "source": [
        "# Creating new target\n",
        "def new_target_fnc(ds, window_size):\n",
        "  l = list()\n",
        "  for i, elem in enumerate(ds):\n",
        "    if (i % window_size) == 0:\n",
        "      l.append(int(elem[1]))\n",
        "    else:\n",
        "      if (i % 2) == 0:\n",
        "        l.append(int(l[i-1] + elem[1]))\n",
        "      else:\n",
        "        l.append(int(l[i-1] - elem[1]))\n",
        "  return l\n",
        "\n",
        "# Preprocessing data\n",
        "def preprocess(data, batch_size, window_size):\n",
        "  new_targets = new_target_fnc(data, window_size)\n",
        "  new_targets = tf.data.Dataset.from_tensor_slices(new_targets)\n",
        "\n",
        "  data = tf.data.Dataset.zip((data, new_targets))\n",
        "  data = data.map(lambda img, new_target: (img[0], new_target))\n",
        "  #data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  data = data.map(lambda img, target: (img, tf.cast(target, tf.float32)))\n",
        "\n",
        "  data = data.map(lambda img, target: (img,target))\n",
        "\n",
        "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  data = data.batch(window_size, drop_remainder=True)\n",
        "  data = data.batch(batch_size, drop_remainder=True)\n",
        "  data = data.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3UZv3_6V6l-"
      },
      "source": [
        "## Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 617,
      "metadata": {
        "id": "Z3coL6r7yZDb"
      },
      "outputs": [],
      "source": [
        "# CNN Model\n",
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer, loss_function, input_shape):\n",
        "    super().__init__()\n",
        "    # input conv1 = 28x28x1\n",
        "    self.conv1 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'), input_shape=input_shape)\n",
        "    # output conv1 = 28x28x24\n",
        "    self.conv2 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv2 = 28x28x24\n",
        "    self.pooling1 = TimeDistributed(AveragePooling2D())\n",
        "    # output pooling1 = 14x14x24\n",
        "    self.conv3 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.conv4 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.globalpooling = TimeDistributed(GlobalAvgPool2D())\n",
        "    # output globalpooling = 7x7x24\n",
        "    self.out = TimeDistributed(Dense(10, activation='softmax'))\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "    self.metrics_list = [\n",
        "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        tf.keras.metrics.Mean(name=\"loss\")\n",
        "    ]\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x, training=False):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  # reset all metrics\n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "          metric.reset_states()\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      image, label = data\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          prediction = self(image, training = True)\n",
        "          loss = self.loss_function(label, prediction)\n",
        "\n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "      self.metrics[0].update_state(label, prediction)\n",
        "      self.metrics[1].update_state(loss)\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "      image, label = data\n",
        "      prediction = self(image, training = False)\n",
        "      loss = self.loss_function(label, prediction)\n",
        "      self.metrics[0].update_state(label, prediction)\n",
        "      self.metrics[1].update_state(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 618,
      "metadata": {
        "id": "Uk_7eCJDAZ3h"
      },
      "outputs": [],
      "source": [
        "# RNN Cell\n",
        "class RNNCell(AbstractRNNCell):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super(RNNCell, self).__init__(**kwargs)\n",
        "    self.units = units\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "      return self.units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      self.kernal = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                    initializer= 'uniform',\n",
        "                                    name= 'kernel')\n",
        "      self.recurrent_kernel = self.add_weight(\n",
        "          shape=(self.units, self.units),\n",
        "          initializer= 'uniform',\n",
        "          name='recurrent_kernel')\n",
        "      self.built = True\n",
        "\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "      previous_output = states[0]\n",
        "      h = backend.dot(inputs, self.kernel)\n",
        "      output = h + backend.dit(previous_output, self.recurrent_kernel)\n",
        "      return output, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 619,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN Model\n",
        "class CNN2(tf.keras.Model):\n",
        "  def __init__(self, input_shape):\n",
        "    super().__init__()\n",
        "    # input conv1 = 28x28x1\n",
        "    self.conv1 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'), input_shape=input_shape)\n",
        "    # output conv1 = 28x28x24\n",
        "    self.conv2 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv2 = 28x28x24\n",
        "    self.pooling1 = TimeDistributed(AveragePooling2D())\n",
        "    # output pooling1 = 14x14x24\n",
        "    self.conv3 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.conv4 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.globalpooling = TimeDistributed(GlobalAvgPool2D())\n",
        "    # output globalpooling = 7x7x24\n",
        "    self.out = TimeDistributed(Dense(1, activation=None))\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x, training=False):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 620,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMCell(AbstractRNNCell):\n",
        "    def __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
        "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
        "\n",
        "        self.hidden_state_units = 20\n",
        "        self.cell_state_units = 20\n",
        "        self.input_size = 1\n",
        "\n",
        "        self.forget_gate = Dense(self.cell_state_units, activation=\"sigmoid\", kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0, seed=None))\n",
        "        self.input_gate = Dense(self.cell_state_units, activation=\"sigmoid\", kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0, seed=None))\n",
        "        self.cell_state_candidates = Dense(self.cell_state_units, activation=\"tanh\", kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0, seed=None))\n",
        "        self.output_gate = Dense(self.hidden_state_units, activation=\"sigmoid\", kernel_initializer=tf.keras.initializers.Orthogonal(gain=1.0, seed=None))\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return [tf.TensorShape([self.hidden_state_units]), \n",
        "                tf.TensorShape([self.cell_state_units])]\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return [tf.TensorShape([self.hidden_state_units])]\n",
        "\n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "        return [tf.zeros(shape=[32,self.hidden_state_units]), \n",
        "                tf.zeros(shape=[32,self.cell_state_units])]\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "\n",
        "        hidden_state = states[0]\n",
        "        cell_state = states[1]\n",
        "\n",
        "        hidden_and_input_concat = tf.concat([hidden_state, tf.convert_to_tensor(inputs)], axis=-1)\n",
        "        hidden_and_input_concat = tf.convert_to_tensor(hidden_and_input_concat)\n",
        "\n",
        "        forget_filter = self.forget_gate(hidden_and_input_concat)\n",
        "        input_filter = self.input_gate(hidden_and_input_concat)\n",
        "        candidate_vector = self.cell_state_candidates(hidden_and_input_concat)\n",
        "\n",
        "        cell_state_with_forgetting = cell_state * forget_filter\n",
        "        cell_state = cell_state_with_forgetting + input_filter * candidate_vector\n",
        "\n",
        "        output_filter = self.output_gate(hidden_and_input_concat)\n",
        "\n",
        "        tanh_of_cell_state = tf.math.tanh(cell_state)\n",
        "        hidden_state = output_filter * tanh_of_cell_state\n",
        "        \n",
        "        return hidden_state, [hidden_state, cell_state]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 621,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self,lstm_cell):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm_cell = lstm_cell\n",
        "\n",
        "        self.lstm_layer = tf.keras.layers.RNN(self.lstm_cell, return_sequences=True, unroll=True)\n",
        "        self.output_layer = Dense(1,activation=None)\n",
        "\n",
        "\n",
        "    def call(self, sequence, training=False):\n",
        "        print(sequence)\n",
        "        for timestep in range(sequence.shape[1]):\n",
        "            rnn_output = self.lstm_layer(sequence)\n",
        "            output = self.output_layer(rnn_output)                 \n",
        "        return output    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 622,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OverallModel(tf.keras.Model):\n",
        "  def __init__(self, cnn, lstm, optimizer, loss_function):\n",
        "    super().__init__()\n",
        "\n",
        "    self.cnn = cnn\n",
        "    self.lstm = lstm\n",
        "\n",
        "    self.metrics_list = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "    \n",
        "  def reset_metrics(self):\n",
        "     for metric in self.metrics:\n",
        "        metric.reset_state()\n",
        "\n",
        "  def call(self, sequence, training = False):\n",
        "    cnn_output = self.cnn(sequence)\n",
        "    lstm_output = self.lstm(cnn_output)\n",
        "    return lstm_output\n",
        "\n",
        "  @tf.function\n",
        "  def training_step(self, data):\n",
        "    image, label = data\n",
        "    print(data)\n",
        "    print(label)\n",
        "    with tf.GradientTape() as tape: \n",
        "      prediction = self(image, training = True)\n",
        "      loss = self.loss_function(label, prediction)\n",
        "\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "    self.metrics[0].update_state(label, prediction)\n",
        "    self.metrics[1].update_state(loss)  \n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "    image, label = data\n",
        "    prediction = self(image, training = False)\n",
        "    loss = self.loss_function(label, prediction)\n",
        "    self.metrics[0].update_state(label, prediction)\n",
        "    self.metrics[1].update_state(loss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 623,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Training LOOp\n",
        "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
        "    for epoch in range (epochs):\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in tqdm(train_ds, position=0, leave=True):\n",
        "            #print(data[1])\n",
        "            #data[1] = data[1][-1]\n",
        "            #print(data[1])\n",
        "\n",
        "            #for x in range(data[0]):\n",
        "            #    print(data[0])\n",
        "            model.train_step(data)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "        \n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in test_ds:\n",
        "            model.test_step(data)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
        "    \n",
        "    model.save_weights(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 624,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train the model\n",
        "\n",
        "batch_size = 32\n",
        "window_size = 4\n",
        "(train_ds,test_ds), ds_info = load_data()\n",
        "train_ds = preprocess(train_ds, batch_size, window_size)\n",
        "test_ds = preprocess(test_ds, batch_size, window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 625,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/468 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [625], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m train_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(train_log_path)\n\u001b[1;32m     15\u001b[0m test_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(test_log_path)\n\u001b[0;32m---> 16\u001b[0m training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\n",
            "Cell \u001b[0;32mIn [623], line 13\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     \u001b[39m#print(data[1])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39m#data[1] = data[1][-1]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39m#for x in range(data[0]):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m#    print(data[0])\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     16\u001b[0m     tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname, model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
            "File \u001b[0;32m~/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/keras/engine/training.py:1024\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m   1023\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1024\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1026\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/keras/engine/training.py:1082\u001b[0m, in \u001b[0;36mModel.compute_loss\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39m\"\"\"Compute the total loss, validate it, and return it.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[1;32m   1033\u001b[0m \u001b[39mSubclasses can optionally override this method to provide custom loss\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[39m  is the case when called by `Model.test_step`).\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mdel\u001b[39;00m x  \u001b[39m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1082\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompiled_loss(\n\u001b[1;32m   1083\u001b[0m     y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlosses\n\u001b[1;32m   1084\u001b[0m )\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ],
      "source": [
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "cnn = CNN2(input_shape=(window_size, 28, 28, 1))\n",
        "lstm_cell = LSTMCell()\n",
        "lstm_layer = tf.keras.layers.RNN(lstm_cell, return_sequences=True, unroll=True)\n",
        "lstm = LSTMModel(lstm_cell=lstm_cell)\n",
        "model = OverallModel(cnn=cnn, lstm=lstm_layer, optimizer=optimizer, loss_function=loss_function)\n",
        "epochs = 10\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "save_path = f\"models/{current_time}\"\n",
        "train_log_path = f\"logs/{current_time}/train\"\n",
        "test_log_path = f\"logs/{current_time}/test\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "window_size = 4\n",
        "(train_ds,test_ds), ds_info = load_data()\n",
        "train_ds = preprocess(train_ds, batch_size, window_size)\n",
        "test_ds = preprocess(test_ds, batch_size, window_size)\n",
        "\n",
        "# for data in train_ds.take(1):\n",
        "    # print(data[0].shape, data[1])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "cnn = CNN2(input_shape=(window_size, 28, 28, 1))\n",
        "lstm = LSTMModel()\n",
        "model = OverallModel(cnn=cnn, lstm=lstm, optimizer=optimizer, loss_function=loss_function)\n",
        "epochs = 10\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "save_path = f\"models/{current_time}\"\n",
        "train_log_path = f\"logs/{current_time}/train\"\n",
        "test_log_path = f\"logs/{current_time}/test\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjUAeujrDquA"
      },
      "outputs": [],
      "source": [
        "# RNN Model\n",
        "class RNNModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(RNNModel, self).__init__()\n",
        "    self.rnn_cell = RNNCell(units=4)\n",
        "        \n",
        "    # return_sequences collects and returns the output \n",
        "    #    of the rnn_cell for all time-steps\n",
        "    # unroll unrolls the network for speed (at the cost of memory)\n",
        "    self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell, \n",
        "                                         return_sequences=True, # we need to know every output in each step\n",
        "                                                                # as we use it for calculation in next state\n",
        "                                         unroll=True) \n",
        "        \n",
        "    self.output_layer = tf.keras.layers.Dense(37, activation=\"softmax\")\n",
        "    \n",
        "        \n",
        "  def call(self, sequence, training=False):\n",
        "        \n",
        "    rnn_output = self.rnn_layer(sequence)\n",
        "        \n",
        "    return self.output_layer(rnn_output)\n",
        "\n",
        "  def train_step(self, data):   \n",
        "    \"\"\"\n",
        "    Standard train_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "    \"\"\"\n",
        "        \n",
        "    sequence, label = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(sequence, training=True)\n",
        "      loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        \n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "     \n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(label, output)\n",
        "        \n",
        "    return {m.name : m.result() for m in self.metrics}\n",
        "    \n",
        "  def test_step(self, data):      \n",
        "    \"\"\"\n",
        "    Standard test_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "    \"\"\"\n",
        "        \n",
        "    sequence, label = data\n",
        "    output = self(sequence, training=False)\n",
        "    loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "                \n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(label, output)\n",
        "        \n",
        "    return {m.name : m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3V7Bw36WB4-"
      },
      "source": [
        "## Prepare Training Loop for CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGDcJfgBAXYT"
      },
      "outputs": [],
      "source": [
        "# Training Loop for CNN model\n",
        "def training_loop(model, train_ds, test_ds, epoch, train_summary_writer, test_summary_writer, save_path):\n",
        "    for epoch in range (epochs):\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in tqdm(train_ds, position=0, leave=True):\n",
        "            model.train_step(data)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "        \n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in test_ds:\n",
        "            model.test_step(data)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
        "    \n",
        "    model.save_weights(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEpvdpiQXObl"
      },
      "source": [
        "## Training the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "06f554fc183d4fb4b8f0783c9b39127d",
            "9e6f90b515404d45a911efdc612830fa",
            "8be0a2807d3f4be297c50f61f383a041",
            "5149677ca6e4457ea72ae94f1288b403",
            "44067b8897914c0a876629e21a36d6d6",
            "f9c5065b730a4ddf975f1b4c3e1a630f",
            "0fdf7df1a1b94cf9b1aeca1ab57c6fcb",
            "a06fe6a3375e4120aafe53521d84df31",
            "45b46edd7af543dd960b931ed9d28f88",
            "a7658b2a3fd941928a10dc3520b08427",
            "1e755d8e65604e8288932f236abb477b"
          ]
        },
        "id": "pKg9wCJo11zO",
        "outputId": "9b074759-1c75-458b-b99b-4b234c30a006"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-29 13:17:53.266321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-29 13:17:53.266478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266550: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266576: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266601: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266626: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266649: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-12-29 13:17:53.266684: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-12-29 13:17:53.268795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/lotta/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/lotta/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "100%|██████████| 468/468 [00:05<00:00, 80.13it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1\n",
            "Loss:  1.4703331 Accuracy:  0.23046875 (Train)\n",
            "Loss:  1.3929701 Accuracy:  0.28265223 (Test)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:05<00:00, 84.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  2\n",
            "Loss:  1.3652712 Accuracy:  0.2860744 (Train)\n",
            "Loss:  1.3628775 Accuracy:  0.28365386 (Test)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 468/468 [00:05<00:00, 89.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  3\n",
            "Loss:  1.3421354 Accuracy:  0.29505542 (Train)\n",
            "Loss:  1.3418858 Accuracy:  0.30158255 (Test)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "window_size = 4\n",
        "(train_ds,test_ds), ds_info = load_data()\n",
        "train_ds = preprocess(train_ds, batch_size, window_size)\n",
        "test_ds = preprocess(test_ds, batch_size, window_size)\n",
        "\n",
        "# for data in train_ds.take(1):\n",
        "    # print(data[0].shape, data[1])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "cnn = CNN(optimizer=optimizer, loss_function=loss_function, input_shape=(window_size, 28, 28, 1))\n",
        "epochs = 3\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "save_path = f\"models/{current_time}\"\n",
        "train_log_path = f\"logs/{current_time}/train\"\n",
        "test_log_path = f\"logs/{current_time}/test\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqE7wqI9pTik",
        "outputId": "0b724101-39e8-4809-f571-0127e609bbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "468/468 [==============================] - 1s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "# Extracting output from CNN model\n",
        "cnn_output_train = cnn.predict(train_ds)\n",
        "\n",
        "# Generating input label from training\n",
        "X_train= list(map(lambda x: x[0], train_ds))\n",
        "y_train= list(map(lambda x: x[1], train_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjFi8xgy2n0B",
        "outputId": "582eb396-e657-488f-daa1-f4f8162fd0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14976, 4, 10)\n",
            "468\n",
            "468\n"
          ]
        }
      ],
      "source": [
        "print(cnn_output_train.shape)\n",
        "print(len(y_train))\n",
        "print(len(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY9WRpifWn0m"
      },
      "outputs": [],
      "source": [
        "# Initiating RNN model\n",
        "rnn = RNNModel()\n",
        "\n",
        "# Compiling the rnn model\n",
        "rnn.compile(optimizer, loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6aqr0oUW38p"
      },
      "outputs": [],
      "source": [
        "# Training RNN Model \n",
        "EXPERIMENT_NAME = \"RNN_model\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{EXPERIMENT_NAME}/{current_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "fcACZuLJaIiW",
        "outputId": "cae38ca8-3ab7-489e-b288-4c97b483637c"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous:\n  x sizes: 14976\n  y sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32\nMake sure all arrays contain the same number of samples.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Training RNN Model using fit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m cnn_output_train,\n\u001b[1;32m      3\u001b[0m                   y \u001b[39m=\u001b[39;49m y_train,\n\u001b[1;32m      4\u001b[0m                   validation_data\u001b[39m=\u001b[39;49mtest_ds,\n\u001b[1;32m      5\u001b[0m                   initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m                   epochs\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m                   callbacks\u001b[39m=\u001b[39;49m[logging_callback])\n",
            "File \u001b[0;32m~/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/keras/engine/data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1842\u001b[0m         label,\n\u001b[1;32m   1843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1844\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1845\u001b[0m         ),\n\u001b[1;32m   1846\u001b[0m     )\n\u001b[1;32m   1847\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 14976\n  y sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32\nMake sure all arrays contain the same number of samples."
          ]
        }
      ],
      "source": [
        "# Training RNN Model using fit\n",
        "history = rnn.fit(x= cnn_output_train,\n",
        "                  y = y_train,\n",
        "                  validation_data=test_ds,\n",
        "                  initial_epoch=2,\n",
        "                  epochs=6,\n",
        "                  callbacks=[logging_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYCvZHtHxUgj"
      },
      "source": [
        "## Plotting the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gO7lh9zxXEz"
      },
      "outputs": [],
      "source": [
        "# Plotting RNN model from History\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.legend(labels=[\"training\",\"validation\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Categorical Crossentropy Loss\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0f0d69c8c30e9dd8d5d9993603a334207417236334c8e6fdf3cd2ae360d39f34"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06f554fc183d4fb4b8f0783c9b39127d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e6f90b515404d45a911efdc612830fa",
              "IPY_MODEL_8be0a2807d3f4be297c50f61f383a041",
              "IPY_MODEL_5149677ca6e4457ea72ae94f1288b403"
            ],
            "layout": "IPY_MODEL_44067b8897914c0a876629e21a36d6d6"
          }
        },
        "0fdf7df1a1b94cf9b1aeca1ab57c6fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e755d8e65604e8288932f236abb477b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44067b8897914c0a876629e21a36d6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45b46edd7af543dd960b931ed9d28f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5149677ca6e4457ea72ae94f1288b403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7658b2a3fd941928a10dc3520b08427",
            "placeholder": "​",
            "style": "IPY_MODEL_1e755d8e65604e8288932f236abb477b",
            "value": " 5/5 [00:00&lt;00:00, 15.69 file/s]"
          }
        },
        "8be0a2807d3f4be297c50f61f383a041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a06fe6a3375e4120aafe53521d84df31",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45b46edd7af543dd960b931ed9d28f88",
            "value": 5
          }
        },
        "9e6f90b515404d45a911efdc612830fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9c5065b730a4ddf975f1b4c3e1a630f",
            "placeholder": "​",
            "style": "IPY_MODEL_0fdf7df1a1b94cf9b1aeca1ab57c6fcb",
            "value": "Dl Completed...: 100%"
          }
        },
        "a06fe6a3375e4120aafe53521d84df31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7658b2a3fd941928a10dc3520b08427": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9c5065b730a4ddf975f1b4c3e1a630f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
