{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Efefefef/IANNwTF-UniOsnabrueck/blob/main/homework07/homework07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWg14xyOL2OG"
      },
      "source": [
        "# HOMEWORK 07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9R9hKeIHyZDX"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D, AbstractRNNCell\n",
        "from keras.initializers import Orthogonal\n",
        "from tqdm import tqdm\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVk0oUzwEgBx"
      },
      "source": [
        "## Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0_1MRMKDHb6f"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "def load_data():\n",
        "    (train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
        "    return (train_ds, test_ds), ds_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FAd7Erb2EQBN"
      },
      "outputs": [],
      "source": [
        "# Creating new target\n",
        "def new_target_fnc(ds, window_size):\n",
        "  l = list()\n",
        "  for i, elem in enumerate(ds):\n",
        "    if (i % window_size) == 0:\n",
        "      l.append(int(elem[1]))\n",
        "    else:\n",
        "      if (i % 2) == 0:\n",
        "        l.append(int(l[i-1] + elem[1]))\n",
        "      else:\n",
        "        l.append(int(l[i-1] - elem[1]))\n",
        "  return l\n",
        "\n",
        "# Preprocessing data\n",
        "def preprocess(data, batch_size, window_size):\n",
        "  new_targets = new_target_fnc(data, window_size)\n",
        "  new_targets = tf.data.Dataset.from_tensor_slices(new_targets)\n",
        "  data = tf.data.Dataset.zip((data, new_targets))\n",
        "  data = data.map(lambda img, new_target: (img[0], new_target))\n",
        "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "\n",
        "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
        "\n",
        "  data = data.batch(window_size, drop_remainder=True)\n",
        "  data = data.batch(batch_size, drop_remainder=True)\n",
        "  data = data.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Model"
      ],
      "metadata": {
        "id": "U3UZv3_6V6l-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z3coL6r7yZDb"
      },
      "outputs": [],
      "source": [
        "# CNN Model\n",
        "class CNN(tf.keras.Model):\n",
        "  def __init__(self, optimizer, loss_function, input_shape):\n",
        "    super().__init__()\n",
        "    # input conv1 = 28x28x1\n",
        "    self.conv1 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'), input_shape=input_shape)\n",
        "    # output conv1 = 28x28x24\n",
        "    self.conv2 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv2 = 28x28x24\n",
        "    self.pooling1 = TimeDistributed(AveragePooling2D())\n",
        "    # output pooling1 = 14x14x24\n",
        "    self.conv3 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.conv4 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
        "    # output conv3 = 14x14x24\n",
        "    self.globalpooling = TimeDistributed(GlobalAvgPool2D())\n",
        "    # output globalpooling = 7x7x24\n",
        "    self.out = TimeDistributed(Dense(10, activation='softmax'))\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "    self.metrics_list = [\n",
        "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        tf.keras.metrics.Mean(name=\"loss\")\n",
        "    ]\n",
        "\n",
        "  @tf.function\n",
        "  def __call__(self, x, training=False):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.pooling1(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "\n",
        "    x = self.globalpooling(x)\n",
        "    x = self.out(x)\n",
        "    return x\n",
        "\n",
        "  # reset all metrics\n",
        "  def reset_metrics(self):\n",
        "      for metric in self.metrics:\n",
        "          metric.reset_states()\n",
        "\n",
        "  @tf.function\n",
        "  def train_step(self, data):\n",
        "      image, label = data\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          prediction = self(image, training = True)\n",
        "          loss = self.loss_function(label, prediction)\n",
        "\n",
        "      gradients = tape.gradient(loss, self.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
        "      self.metrics[0].update_state(label, prediction)\n",
        "      self.metrics[1].update_state(loss)\n",
        "\n",
        "  @tf.function\n",
        "  def test_step(self, data):\n",
        "      image, label = data\n",
        "      prediction = self(image, training = False)\n",
        "      loss = self.loss_function(label, prediction)\n",
        "      self.metrics[0].update_state(label, prediction)\n",
        "      self.metrics[1].update_state(loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Cell\n",
        "class RNNCell(AbstractRNNCell):\n",
        "  def __init__(self, rec_unit_1, rec_unit_2, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.rec_unit_1 = rec_unit_1\n",
        "    self.rec_unit_2 = rec_unit_2\n",
        "\n",
        "    self.linear_1 = Dense(rec_unit_1)\n",
        "    self.linear_2 = Dense(rec_unit_2)\n",
        "\n",
        "    # 1st recurrent layer \n",
        "    self.rec_layer_1 = Dense(rec_unit_1, \n",
        "                             kernel_initializer= Orthogonal(gain=1, seed=None),\n",
        "                             activation= tf.nn.tanh)\n",
        "    \n",
        "    # layer normalisation for trainability\n",
        "    self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    # 2nd recurrent layer\n",
        "    self.rec_layer_2 = Dense(rec_unit_2, \n",
        "                             kernel_initializer= Orthogonal(gain=1, seed=None),\n",
        "                             activation= tf.nn.tanh)\n",
        "    \n",
        "    # layer normalisation for trainability\n",
        "    self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return [tf.TensorShape([self.rec_unit_1]),\n",
        "                tf.TensorShape([self.rec_unit_2])]\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return [tf.TensorShape([self.rec_unit_2])]\n",
        "    \n",
        "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
        "        return [tf.zeros([self.rec_unit_1]), \n",
        "                tf.zeros([self.rec_unit_2])]\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # unpack the states\n",
        "        state_layer_1 = states[0]\n",
        "        state_layer_2 = states[1]\n",
        "        \n",
        "        # linearly project input\n",
        "        x = self.linear_1(inputs) + state_layer_1\n",
        "        \n",
        "        # apply first recurrent kernel\n",
        "        new_state_layer_1 = self.rec_layer_1(x)\n",
        "        \n",
        "        # apply layer norm\n",
        "        x = self.layer_norm_1(new_state_layer_1)\n",
        "        \n",
        "        # linearly project output of layer norm\n",
        "        x = self.linear_2(x) + state_layer_2\n",
        "        \n",
        "        # apply second recurrent layer\n",
        "        new_state_layer_2 = self.rec_layer_2(x)\n",
        "        \n",
        "        # apply second layer's layer norm\n",
        "        x = self.layer_norm_2(new_state_layer_2)\n",
        "        \n",
        "        # return output and the list of new states of the layers\n",
        "        return x, [new_state_layer_1, new_state_layer_2]\n",
        "    \n",
        "    def get_config(self):\n",
        "        return {\"recurrent_units_1\": self.rec_unit_1, \n",
        "                \"recurrent_units_2\": self.rec_unit_2}"
      ],
      "metadata": {
        "id": "Uk_7eCJDAZ3h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Model\n",
        "class RNNModel(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "        \n",
        "    self.rnn_cell = RNNCell(rec_unit_1=24,\n",
        "                            rec_unit_2=48)\n",
        "        \n",
        "    # return_sequences collects and returns the output \n",
        "    #    of the rnn_cell for all time-steps\n",
        "    # unroll unrolls the network for speed (at the cost of memory)\n",
        "    self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell, \n",
        "                                         return_sequences=True, # we need to know every output in each step\n",
        "                                                                # as we use it for calculation in next state\n",
        "                                         unroll=True) \n",
        "        \n",
        "    self.output_layer = tf.keras.layers.Dense(37, activation=\"softmax\")\n",
        "    self.metrics_list = [\n",
        "                 tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "                 tf.keras.metrics.Mean(name=\"loss\")]\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return self.metrics_list\n",
        "    \n",
        "  def reset_metrics(self):\n",
        "     for metric in self.metrics:\n",
        "       metric.reset_state()\n",
        "        \n",
        "  def call(self, sequence, training=False):\n",
        "        \n",
        "    rnn_output = self.rnn_layer(sequence)\n",
        "        \n",
        "    return self.output_layer(rnn_output)\n",
        "\n",
        "  def train_step(self, data):   \n",
        "    \"\"\"\n",
        "    Standard train_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "    \"\"\"\n",
        "        \n",
        "    sequence, label = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = self(sequence, training=True)\n",
        "      loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        \n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "     \n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(label, output)\n",
        "        \n",
        "    return {m.name : m.result() for m in self.metrics}\n",
        "    \n",
        "  def test_step(self, data):      \n",
        "    \"\"\"\n",
        "    Standard test_step method, assuming we use model.compile(optimizer, loss, ...)\n",
        "    \"\"\"\n",
        "        \n",
        "    sequence, label = data\n",
        "    output = self(sequence, training=False)\n",
        "    loss = self.compiled_loss(label, output, regularization_losses=self.losses)\n",
        "                \n",
        "    self.metrics[0].update_state(loss)\n",
        "    self.metrics[1].update_state(label, output)\n",
        "        \n",
        "    return {m.name : m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "hjUAeujrDquA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Training Loop for CNN"
      ],
      "metadata": {
        "id": "d3V7Bw36WB4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop for CNN model\n",
        "def training_loop(model, train_ds, test_ds, epoch, train_summary_writer, test_summary_writer, save_path):\n",
        "    for epoch in range (epochs):\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in tqdm(train_ds, position=0, leave=True):\n",
        "            model.train_step(data)\n",
        "\n",
        "        with train_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "        \n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
        "        model.reset_metrics()\n",
        "\n",
        "        for data in test_ds:\n",
        "            model.test_step(data)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
        "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
        "\n",
        "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
        "    \n",
        "    model.save_weights(save_path)"
      ],
      "metadata": {
        "id": "QGDcJfgBAXYT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the CNN model"
      ],
      "metadata": {
        "id": "EEpvdpiQXObl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pKg9wCJo11zO",
        "outputId": "672976c5-dee0-45dc-de91-c058c52bb2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [01:13<00:00,  6.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n",
            "Loss:  1.4767942 Accuracy:  0.23584402 (Train)\n",
            "Loss:  1.3960503 Accuracy:  0.2996795 (Test)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [01:01<00:00,  7.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  2\n",
            "Loss:  1.3744901 Accuracy:  0.2852898 (Train)\n",
            "Loss:  1.3652352 Accuracy:  0.2960737 (Test)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 468/468 [01:12<00:00,  6.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  3\n",
            "Loss:  1.3453008 Accuracy:  0.2946214 (Train)\n",
            "Loss:  1.3499708 Accuracy:  0.29757613 (Test)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "window_size = 4\n",
        "(train_ds,test_ds), ds_info = load_data()\n",
        "train_ds = preprocess(train_ds, batch_size, window_size)\n",
        "test_ds = preprocess(test_ds, batch_size, window_size)\n",
        "\n",
        "# for data in train_ds.take(1):\n",
        "    # print(data[0].shape, data[1])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "cnn = CNN(optimizer=optimizer, loss_function=loss_function, input_shape=(window_size, 28, 28, 1))\n",
        "epochs = 3\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "save_path = f\"models/{current_time}\"\n",
        "train_log_path = f\"logs/{current_time}/train\"\n",
        "test_log_path = f\"logs/{current_time}/test\"\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
        "training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting output from CNN model\n",
        "cnn_output_train = cnn.predict(train_ds)\n",
        "\n",
        "# Generating input label from training\n",
        "X_train= list(map(lambda x: x[0], train_ds))\n",
        "y_train= list(map(lambda x: x[1], train_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqE7wqI9pTik",
        "outputId": "3ca1bae9-e362-44a4-ee5a-439b4b12e2d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468/468 [==============================] - 21s 44ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_output_train.shape)\n",
        "print(len(y_train))\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjFi8xgy2n0B",
        "outputId": "d2931d9f-bfa8-4e97-ab11-69dcff972b34"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14976, 4, 10)\n",
            "468\n",
            "468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiating RNN model\n",
        "rnn = RNNModel()\n",
        "\n",
        "# Compiling the rnn model\n",
        "rnn.compile(optimizer, loss_function)"
      ],
      "metadata": {
        "id": "TY9WRpifWn0m"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RNN Model \n",
        "EXPERIMENT_NAME = \"RNN_model\"\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logging_callback = tf.keras.callbacks.TensorBoard(log_dir=f\"./logs/{EXPERIMENT_NAME}/{current_time}\")"
      ],
      "metadata": {
        "id": "s6aqr0oUW38p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RNN Model using fit\n",
        "history = rnn.fit(x= cnn_output_train,\n",
        "                  y = y_train,\n",
        "                  validation_data=test_ds,\n",
        "                  initial_epoch=2,\n",
        "                  epochs=6,\n",
        "                  callbacks=[logging_callback])"
      ],
      "metadata": {
        "id": "fcACZuLJaIiW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "2b47cc99-9ab0-445d-94cf-5d2855b46bed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e0d42fef643b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training RNN Model using fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = rnn.fit(x= cnn_output_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1653\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1654\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 14976\n  y sizes: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32...\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the result"
      ],
      "metadata": {
        "id": "TYCvZHtHxUgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting RNN model from History\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.legend(labels=[\"training\",\"validation\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Categorical Crossentropy Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3gO7lh9zxXEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0f0d69c8c30e9dd8d5d9993603a334207417236334c8e6fdf3cd2ae360d39f34"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}