{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWg14xyOL2OG"
   },
   "source": [
    "# HOMEWORK 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVk0oUzwEgBx"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0_1MRMKDHb6f"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_data():\n",
    "    (train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "    return (train_ds, test_ds), ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FAd7Erb2EQBN"
   },
   "outputs": [],
   "source": [
    "def new_target_fnc(ds, window_size):\n",
    "  l = list()\n",
    "  for i, elem in enumerate(ds):\n",
    "    if (i % window_size) == 0:\n",
    "      l.append(int(elem[1]))\n",
    "    else:\n",
    "      if (i % 2) == 0:\n",
    "        l.append(int(l[i-1] + elem[1]))\n",
    "      else:\n",
    "        l.append(int(l[i-1] - elem[1]))\n",
    "  return l\n",
    "\n",
    "def preprocess(data, batch_size, window_size):\n",
    "  new_targets = new_target_fnc(data, window_size)\n",
    "  new_targets = tf.data.Dataset.from_tensor_slices(new_targets)\n",
    "  data = tf.data.Dataset.zip((data, new_targets))\n",
    "  data = data.map(lambda img, new_target: (img[0], new_target))\n",
    "\n",
    "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
    "\n",
    "  data = data.batch(window_size, drop_remainder=True)\n",
    "  data = data.batch(batch_size, drop_remainder=True)\n",
    "  data = data.cache() \n",
    "  data = data.shuffle(1000) # Does it shuffle whole points or windows?\n",
    "  data = data.prefetch(tf.data.AUTOTUNE)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "  def __init__(self, optimizer, loss_function, input_shape):\n",
    "    super().__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(24, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv2 = tf.keras.layers.Conv2D(24, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.pooling1 = tf.keras.layers.TimeDistributed(tf.keras.layers.AveragePooling2D())\n",
    "    self.conv3 = tf.keras.layers.Conv2D(48, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv4 = tf.keras.layers.Conv2D(48, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.pooling2 = tf.keras.layers.TimeDistributed(tf.keras.layers.AveragePooling2D())\n",
    "    self.conv5 = tf.keras.layers.Conv2D(96, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.conv6 = tf.keras.layers.Conv2D(96, 3, activation='relu', padding='same', input_shape=input_shape[2:])\n",
    "    self.globalpooling = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAvgPool2D())\n",
    "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "    self.metrics_list = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.Mean(name=\"loss\")\n",
    "    ]\n",
    "\n",
    "  def call(self, x, training=False):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pooling1(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "    x = self.pooling2(x)\n",
    "    x = self.conv5(x)\n",
    "    x = self.conv6(x)\n",
    "    x = self.globalpooling(x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "\n",
    "  # reset all metrics\n",
    "  def reset_metrics(self):\n",
    "      for metric in self.metrics:\n",
    "          metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "      image, label = data\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "          prediction = self(image, training = True)\n",
    "          loss = self.loss_function(label, prediction)\n",
    "\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "      image, label = data\n",
    "      prediction = self(image, training = False)\n",
    "      loss = self.loss_function(label, prediction)\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range (epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            model.train_step(data)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in test_ds:\n",
    "            model.test_step(data)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "    \n",
    "    model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pKg9wCJo11zO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/468 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_58494/1230670204.py\", line 47, in train_step  *\n        loss = self.loss_function(label, prediction)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 4) and (32, 4, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m train_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(train_log_path)\n\u001b[1;32m     20\u001b[0m test_summary_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mcreate_file_writer(test_log_path)\n\u001b[0;32m---> 21\u001b[0m training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\n",
      "Cell \u001b[0;32mIn[38], line 68\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m model\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_ds, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 68\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     71\u001b[0m     tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname, model\u001b[39m.\u001b[39mmetrics[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/var/folders/wt/xy7579_50v90hx4jnj0017rc0000gn/T/ipykernel_58494/1230670204.py\", line 47, in train_step  *\n        loss = self.loss_function(label, prediction)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 141, in __call__  **\n        losses = call_fn(y_true, y_pred)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/opt/miniconda3/envs/scipy/lib/python3.10/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 4) and (32, 4, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "window_size = 4\n",
    "(train_ds,test_ds), ds_info = load_data()\n",
    "train_ds = preprocess(train_ds, batch_size, window_size)\n",
    "test_ds = preprocess(test_ds, batch_size, window_size)\n",
    "\n",
    "# for data in train_ds.take(1):\n",
    "    # print(data[0].shape, data[1])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "cnn = CNN(optimizer=optimizer, loss_function=loss_function, input_shape=(batch_size, window_size, 28, 28, 1))\n",
    "epochs = 10\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"models/{current_time}\"\n",
    "train_log_path = f\"logs/{current_time}/train\"\n",
    "test_log_path = f\"logs/{current_time}/test\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('scipy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6afa05d019bf0021cfdd951a280579caaf080683501157ada207acf29e18cb37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
