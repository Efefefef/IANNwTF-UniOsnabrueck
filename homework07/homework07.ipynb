{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWg14xyOL2OG"
   },
   "source": [
    "# HOMEWORK 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:16:57.296541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 22:16:57.605022: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-19 22:16:57.610922: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-19 22:16:57.610931: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-19 22:16:58.171817: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 22:16:58.171855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 22:16:58.171858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/lotta/anaconda3/envs/wtf-gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv2D, AveragePooling2D, TimeDistributed, LSTM, GlobalAvgPool2D\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVk0oUzwEgBx"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0_1MRMKDHb6f"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "def load_data():\n",
    "    (train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "    return (train_ds, test_ds), ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FAd7Erb2EQBN"
   },
   "outputs": [],
   "source": [
    "def new_target_fnc(ds, window_size):\n",
    "  l = list()\n",
    "  for i, elem in enumerate(ds):\n",
    "    if (i % window_size) == 0:\n",
    "      l.append(int(elem[1]))\n",
    "    else:\n",
    "      if (i % 2) == 0:\n",
    "        l.append(int(l[i-1] + elem[1]))\n",
    "      else:\n",
    "        l.append(int(l[i-1] - elem[1]))\n",
    "  return l\n",
    "\n",
    "def preprocess(data, batch_size, window_size):\n",
    "  new_targets = new_target_fnc(data, window_size)\n",
    "  new_targets = tf.data.Dataset.from_tensor_slices(new_targets)\n",
    "  data = tf.data.Dataset.zip((data, new_targets))\n",
    "  data = data.map(lambda img, new_target: (img[0], new_target))\n",
    "  data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "  data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "  data = data.map(lambda img, target: ((img/128.)-1., target))\n",
    "\n",
    "  data = data.batch(window_size, drop_remainder=True)\n",
    "  data = data.batch(batch_size, drop_remainder=True)\n",
    "  data = data.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "  def __init__(self, optimizer, loss_function, input_shape):\n",
    "    super().__init__()\n",
    "    self.conv1 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'), input_shape=input_shape)\n",
    "    self.conv2 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
    "    self.pooling1 = TimeDistributed(AveragePooling2D())\n",
    "    self.conv3 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
    "    self.conv4 = TimeDistributed(Conv2D(24, 3, activation='relu', padding='valid'))\n",
    "    \n",
    "    self.globalpooling = TimeDistributed(GlobalAvgPool2D())\n",
    "    self.out = TimeDistributed(Dense(10, activation='softmax'))\n",
    "\n",
    "    self.optimizer = optimizer\n",
    "    self.loss_function = loss_function\n",
    "\n",
    "    self.metrics_list = [\n",
    "        tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.Mean(name=\"loss\")\n",
    "    ]\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, x, training=False):\n",
    "    x = self.conv1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pooling1(x)\n",
    "    x = self.conv3(x)\n",
    "    x = self.conv4(x)\n",
    "\n",
    "    x = self.globalpooling(x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "\n",
    "  # reset all metrics\n",
    "  def reset_metrics(self):\n",
    "      for metric in self.metrics:\n",
    "          metric.reset_states()\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "      image, label = data\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "          prediction = self(image, training = True)\n",
    "          loss = self.loss_function(label, prediction)\n",
    "\n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients,self.trainable_variables))\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "      image, label = data\n",
    "      prediction = self(image, training = False)\n",
    "      loss = self.loss_function(label, prediction)\n",
    "      self.metrics[0].update_state(label, prediction)\n",
    "      self.metrics[1].update_state(loss)\n",
    "\n",
    "\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path):\n",
    "    for epoch in range (epochs):\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in tqdm(train_ds, position=0, leave=True):\n",
    "            model.train_step(data)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "        \n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Train)\")\n",
    "        model.reset_metrics()\n",
    "\n",
    "        for data in test_ds:\n",
    "            model.test_step(data)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar(model.metrics[0].name, model.metrics[0].result(), step=epoch)\n",
    "            tf.summary.scalar(model.metrics[1].name, model.metrics[1].result(), step=epoch)\n",
    "\n",
    "        print(\"Loss: \", model.metrics[1].result().numpy(), \"Accuracy: \", model.metrics[0].result().numpy(), \"(Test)\")\n",
    "    \n",
    "    model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pKg9wCJo11zO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:06<00:00, 73.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Loss:  1.4735317 Accuracy:  0.23754674 (Train)\n",
      "Loss:  1.3944105 Accuracy:  0.27323717 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 92.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Loss:  1.3679693 Accuracy:  0.28432158 (Train)\n",
      "Loss:  1.3539364 Accuracy:  0.29296875 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 91.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "Loss:  1.3409785 Accuracy:  0.29627404 (Train)\n",
      "Loss:  1.3421007 Accuracy:  0.29897836 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 88.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "Loss:  1.3280296 Accuracy:  0.30198318 (Train)\n",
      "Loss:  1.329325 Accuracy:  0.30769232 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 86.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "Loss:  1.3201042 Accuracy:  0.3047042 (Train)\n",
      "Loss:  1.325019 Accuracy:  0.31059694 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 87.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n",
      "Loss:  1.3144598 Accuracy:  0.3057058 (Train)\n",
      "Loss:  1.3232454 Accuracy:  0.30929488 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:05<00:00, 87.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n",
      "Loss:  1.3096493 Accuracy:  0.30805957 (Train)\n",
      "Loss:  1.3203253 Accuracy:  0.3116987 (Test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 212/468 [00:02<00:02, 89.35it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "window_size = 4\n",
    "(train_ds,test_ds), ds_info = load_data()\n",
    "train_ds = preprocess(train_ds, batch_size, window_size)\n",
    "test_ds = preprocess(test_ds, batch_size, window_size)\n",
    "\n",
    "# for data in train_ds.take(1):\n",
    "    # print(data[0].shape, data[1])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "cnn = CNN(optimizer=optimizer, loss_function=loss_function, input_shape=(window_size, 28, 28, 1))\n",
    "epochs = 10\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_path = f\"models/{current_time}\"\n",
    "train_log_path = f\"logs/{current_time}/train\"\n",
    "test_log_path = f\"logs/{current_time}/test\"\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_path)\n",
    "training_loop(cnn, train_ds, test_ds, epochs, train_summary_writer, test_summary_writer, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f0d69c8c30e9dd8d5d9993603a334207417236334c8e6fdf3cd2ae360d39f34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
